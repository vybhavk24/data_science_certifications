# ML_C3_M1

## What Is Clustering?

### Prerequisites

- Linear algebra: vectors in (\mathbb{R}^n)
- Distance metrics: Euclidean, Manhattan, cosine similarity
- Concept of unsupervised learning (no labeled outputs)

Quick refresher on Euclidean distance:

```python
def euclidean_distance(a, b):
    # a, b are arrays of shape (n,)
    return np.sqrt(np.sum((a - b) ** 2))
```

### Intuition & Analogy

Imagine you dump a box of mixed LEGO bricks on the floor and want to group them by color or shape—without any instruction manual. Clustering is the algorithmic version of that:

- You have unlabeled data points
- You want to discover “natural” groups
- You rely on a notion of similarity or distance

### Formal Definition

Given a dataset

```python
X = [x(1), x(2), …, x(m)]   # each x(i) ∈ ℝⁿ
```

clustering partitions X into K disjoint subsets (clusters)

```python
C = {C1, C2, …, CK}
# where ⋃_{j=1..K} Cj = X  and  Ci ∩ Cj = ∅  for i≠j
```

Each cluster groups points more similar to each other than to points in other clusters.

Hard vs. soft clustering:

- Hard: each point belongs to exactly one cluster
- Soft (fuzzy): each point has membership weights across clusters

### Main Clustering Paradigms

1. Partitional clustering
    - Fixed number of clusters K
    - Example: K-means, K-medoids
2. Hierarchical clustering
    - Builds a tree (dendrogram) of clusters
    - Agglomerative (bottom-up) or divisive (top-down)
3. Density-based clustering
    - Finds arbitrarily shaped clusters based on point density
    - Example: DBSCAN
4. Model-based clustering
    - Assumes data generated by mixture of probability distributions
    - Example: Gaussian Mixture Models (GMM)

### Core Math Concepts

### Distance & Similarity

- **Euclidean distance** between x and y:
    
    ```python
    dist = np.linalg.norm(x - y)
    ```
    
- **Manhattan distance**:
    
    ```python
    dist = np.sum(np.abs(x - y))
    ```
    
- **Cosine similarity** (higher = more similar):
    
    ```python
    sim = (x @ y) / (np.linalg.norm(x) * np.linalg.norm(y))
    ```
    

### Cluster Compactness (Illustrative Cost)

For partitional methods you often minimize within-cluster variance. In K-means, for example:

```python
J = (1/m) * sum_{i=1..m} || x(i) - μ_{c(i)} ||**2
```

- μ_j is the centroid of cluster j
- c(i) is the index of the cluster assigned to x(i)

### Real-World ML Use Cases

- Customer segmentation in marketing (tailor campaigns)
- Image segmentation (group pixels into objects/background)
- Document clustering (group news articles by topic)
- Network analysis (community detection in social graphs)
- Preprocessing: compress features or detect outliers

### Geometric & Visual Insights

- In 2D, clusters appear as dense “islands” of points.
- Partitional methods (K-means) carve space into Voronoi regions around centroids.
- Hierarchical clustering can be visualized as a dendrogram showing merge/split steps.

### Practice Problems

1. Pairwise Distances
    - Create 5 random 2D points
    - Compute the 5×5 distance matrix using Euclidean and Manhattan metrics
    - Plot the points and annotate distances on the plot
2. Manual Agglomerative Clustering
    - Take your 5 points and repeatedly merge the two closest clusters until one remains
    - Record merge steps (dendrogram) by hand or using SciPy’s `linkage` and `dendrogram`
3. Discover K with Elbow Method
    - Generate 300 points from three Gaussian blobs (use `sklearn.datasets.make_blobs`)
    - For K from 1 to 6, run K-means (from scikit-learn) and record inertia (within-cluster sum of squares)
    - Plot inertia vs K and identify the “elbow”

Hints:

- Use `sklearn.cluster.KMeans(initialize='k-means++')` for better initialization
- Inertia is accessible via `model.inertia_`

---

## K-Means Intuition

### Prerequisites

- Euclidean distance
- Vector arithmetic (addition, averaging)
- Basic Python & NumPy

### 1. High-Level Analogy

Imagine three magnets scattered under a thin sheet of metal. You drop iron filings (data points) on top. Each filing is attracted to the nearest magnet. Over time, you slide each magnet so it sits at the center of its filings. After a few adjustments, filings settle into three tight groups around the magnets.

K-means works the same way: centroids (magnets) adjust to minimize “pull” from assigned points.

### 2. Step-by-Step Intuition

1. **Initial Guess**
    - Pick K random centroids.
    - At first, they may lie far from actual clusters.
2. **Assignment Step**
    - Each point finds its nearest centroid.
    - This carves the space into Voronoi cells—regions where one centroid “owns” all points inside.
3. **Update Step**
    - For each centroid, recompute its location as the average of its assigned points.
    - Centroids move toward dense regions of points.
4. **Repeat**
    - Reassign points to the nearest updated centroids.
    - Move centroids again.
    - Continue until assignments stop changing (or move very little).

Every assignment and update strictly lowers the total squared distance from points to their centroids, so the algorithm converges to a local minimum.

### 3. Distortion Cost Function

K-means is effectively minimizing this cost:

```python
# X: array of shape (m, n)
# mu: array of shape (K, n)
# c: array of shape (m,) where c[i] is index of centroid for X[i]

J = (1 / m) * sum(
    np.sum((X[i] - mu[c[i]])**2)
    for i in range(m)
)
```

- J measures total within-cluster variance.
- Assignment lowers J by picking closer centroids.
- Update lowers J by relocating centroids to the mean of assigned points.

### 4. Why K-Means “Finds” Clusters

- **Local Attraction**: Points “pull” centroids toward dense pockets.
- **Voronoi Partitioning**: Space is split so each centroid influences its own region.
- **Mean as Optimal Representative**: For squared-distance loss, the mean minimizes average squared distance.

Together, these guarantee every iteration moves centroids closer to true cluster centers—until no further improvement is possible.

### 5. Geometric Visualization

- In 2D, draw centroids and connect each point to its assigned centroid.
- Observe how centroids shift toward the center of star-like point clouds.
- Voronoi boundaries are straight lines equidistant from two centroids.

### 6. Practical Considerations

- **Initialization Sensitivity**: Bad random seeds can trap you in poor local minima.
    - Use K-means++ to smartly seed centroids.
- **Choosing K**: No universal rule—use Elbow Method or silhouette scores.
- **Feature Scaling**: Always normalize features so distances are meaningful.
- **Empty Clusters**: If a centroid gets no points, reinitialize it to a random point.

### 7. Real-World Workflow

1. **Preprocess**
    - Scale each feature to zero mean & unit variance.
2. **Initialize & Run**
    - Use K-means++ for centroids.
    - Fit to your data.
3. **Evaluate**
    - Plot inertia vs K (Elbow).
    - Compute silhouette scores for cluster separation.
4. **Apply**
    - Use cluster labels as new categorical features in downstream models.
    - Segment customers, compress colors, detect groups in graphs.

### 8. Practice Problems

1. **Visualizing Centroid Movement**
    - Generate 200 2D points from 3 blobs (`make_blobs`).
    - Implement K-means with `plot` at each iteration: show centroids and their Voronoi regions.
2. **K-Means++ vs Random Init**
    - Run standard random init and K-means++ on the same dataset.
    - Plot final distortions; compare convergence speed and final J.
3. **Cluster-As-Feature**
    - On the Iris dataset, run K-means for K=3.
    - Append cluster labels to the original feature set.
    - Train a simple classifier (e.g., logistic regression) using features+cluster; compare to using features alone.

---

## K-means Algorithm

### Prerequisites

- Euclidean distance and vector arithmetic
- Concepts of optimization and coordinate descent
- Basic Python and NumPy for implementation

### 1. Problem Definition

K-means seeks to partition m unlabeled points X into K clusters by finding both

cluster assignments and centroids that minimize total within-cluster variance.

Clusters are defined by:

```python
X = array of shape (m, n)       # data points
c = integer array of shape (m,) # cluster index for each point
mu = array of shape (K, n)      # K centroids in R^n
```

### 2. Cost Function

The objective J measures squared distance from each point to its assigned centroid:

```python
J = (1 / m) * sum(
    np.sum((X[i] - mu[c[i]])**2)
    for i in range(m)
)
```

- X[i] is the i-th data point
- c[i] is its cluster index (0 ≤ c[i] < K)
- mu[j] is the j-th centroid

K-means minimizes J by alternating assignment and update steps.

### 3. Core Algorithm Steps

1. **Initialize centroids**
    - Randomly select K points from X
    - Or use K-means++ for smarter seeding
2. **Assignment step**
    
    For each point X[i], pick the closest centroid:
    
    ```python
    c[i] = argmin_j np.linalg.norm(X[i] - mu[j])**2
    ```
    
3. **Update step**
    
    For each cluster j, move centroid to mean of its assigned points:
    
    ```python
    mu[j] = X[c == j].mean(axis=0)
    ```
    
4. **Repeat**
    - Alternate assignment and update
    - Stop when assignments no longer change or after max iterations

Each iteration never increases J, so the algorithm converges to a local minimum.

### 4. Initialization Strategies

- **Random init**
    
    Choose K examples from X without replacement. Simple but can lead to poor minima.
    
- **K-means++**
    1. Pick first centroid uniformly at random from X.
    2. For each remaining centroid, compute D(x) = squared distance to nearest chosen centroid.
    3. Sample next centroid from X with probability ∝ D(x).
    4. Repeat until K centroids are chosen.

K-means++ often yields faster convergence and better clustering.

### 5. Convergence and Complexity

- **Convergence**
    
    Each assignment/update pair reduces J or leaves it unchanged. Converges in a finite number of steps to a local minimum.
    
- **Time complexity**
    
    O(K · m · n · T) where:
    
    - m = number of points
    - n = number of features
    - K = number of clusters
    - T = iterations until convergence
- **Local minima**
    
    Multiple restarts with different initial seeds help avoid poor solutions.
    

### 6. Vectorized Python Implementation

```python
import numpy as np

def kmeans(X, K, max_iters=100):
    m, n = X.shape
    # K-means++ initialization
    centroids = np.empty((K, n))
    centroids[0] = X[np.random.randint(m)]
    for k in range(1, K):
        dist_sq = np.min(np.linalg.norm(X - centroids[:k], axis=1)**2, axis=0)
        probs = dist_sq / dist_sq.sum()
        centroids[k] = X[np.random.choice(m, p=probs)]

    for _ in range(max_iters):
        # Assignment
        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
        c = np.argmin(distances, axis=1)
        # Update
        new_centroids = np.array([
            X[c == j].mean(axis=0) if np.any(c == j) else centroids[j]
            for j in range(K)
        ])
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids

    return centroids, c
```

### 7. Practical Tips and Pitfalls

- **Feature scaling**
    
    Always standardize features so distances are comparable.
    
- **Empty clusters**
    
    If no points assign to a centroid, reinitialize it to a random point.
    
- **Choosing K**
    
    Use the Elbow Method (inertia vs K) or silhouette score.
    
- **Mini-batch K-means**
    
    Speeds up large datasets by using small random batches per update.
    
- **Outlier sensitivity**
    
    K-means uses squared distance; outliers can distort centroids. Consider trimming or robust variants.
    

### 8. Real-World ML Workflow

1. **Preprocess** your data (clean, scale).
2. **Select K** with domain knowledge or exploratory methods.
3. **Run K-means++** for initialization and cluster assignment.
4. **Evaluate clusters** using inertia, silhouette, or downstream model performance.
5. **Use cluster labels** as features for supervised models or for segmentation tasks.

### 9. Practice Problems

1. Visualize K-means on 2D blobs with animated iterations showing centroids and Voronoi cells.
2. Compare random init vs K-means++ on a synthetic dataset and plot convergence of J over iterations.
3. Apply mini-batch K-means on a large image (e.g., 500×500 pixels) for color compression and measure speedup.

---

## Optimization Objective

### 1. Definition of the Cost Function

K-means clusters by minimizing the total within-cluster squared distance (often called distortion or inertia). Formally, for data

```python
X = [x(1), x(2), …, x(m)]       # m examples in Rⁿ
mu = [mu(1), mu(2), …, mu(K)]   # K centroids in Rⁿ
c = [c(1), c(2), …, c(m)]       # assignment: c(i) ∈ {1…K}
```

the objective J is:

```python
J = sum(
    squared_distance(x(i), mu[c(i)])
    for i in range(m)
)
```

where

```python
def squared_distance(a, b):
    return np.sum((a - b)**2)
```

### 2. Breakdown of Variables

- **X[i]**: the i-th training example, a vector in ℝⁿ
- **mu[j]**: the j-th centroid, a vector in ℝⁿ
- **c[i]**: index of the centroid assigned to X[i]
- **K**: number of clusters
- **J**: total distortion across all points

We often normalize by m to compare across datasets:

```python
J_normalized = J / m
```

### 3. Why This Objective?

- **Mean Minimizes Squared Error**: For squared-distance loss, the average (mean) of points in a cluster is the optimal representative.
- **Compact Clusters**: Minimizing J forces points to be as close as possible to their assigned centroid, leading to tight, well-defined groups.
- **Geometric Interpretation**: J is the sum of squared radii of K “spheres” (clusters). Lower J means smaller, denser spheres.

### 4. Minimizing J via Coordinate Descent

K-means uses an alternating minimization (coordinate descent) strategy:

1. **Assignment Step (fix centroids, optimize c)**
    
    ```python
    for i in range(m):
        c[i] = argmin_j squared_distance(X[i], mu[j])
    ```
    
    This chooses the best cluster index for each point, decreasing J or leaving it unchanged.
    
2. **Update Step (fix assignments, optimize mu)**
    
    ```python
    for j in range(K):
        members = X[c == j]
        if len(members) > 0:
            mu[j] = members.mean(axis=0)
    ```
    
    Setting each mu[j] to the mean of its cluster members minimizes the sum of squared distances within that cluster.
    

Repeated assignment and update never increase J and converge to a local minimum.

### 5. Properties of the Objective

- **Non-Convex**: J has many local minima; global minimization is NP-hard in general.
- **Local Convergence**: K-means always converges in a finite number of steps to a local optimum.
- **Initialization Sensitive**: Different seeds yield different local minima. Multiple restarts or K-means++ mitigate poor solutions.
- **Scale Dependence**: Feature scaling directly affects J; always normalize or standardize features first.

### 6. Relation to Gaussian Mixture Models

Under a GMM with equal, isotropic covariances σ²I and uniform mixing weights, the maximum-likelihood estimate of cluster centers coincides with the minimizer of J. K-means can thus be viewed as a limiting case of EM for GMMs as σ² → 0.

### 7. Computing J in Code

Here’s a simple function to compute distortion J and track its value over iterations:

```python
import numpy as np

def compute_distortion(X, centroids, assignments):
    # X: (m, n), centroids: (K, n), assignments: (m,)
    return np.sum([
        np.sum((X[i] - centroids[assignments[i]])**2)
        for i in range(X.shape[0])
    ])

def kmeans_with_history(X, K, max_iters=100):
    # initialize centroids (random or kmeans++)
    centroids = X[np.random.choice(len(X), K, replace=False)]
    history = []
    for _ in range(max_iters):
        # assignment
        dists = np.linalg.norm(X[:, None] - centroids, axis=2)**2
        c = np.argmin(dists, axis=1)
        # update
        new_centroids = np.array([
            X[c == j].mean(axis=0) if np.any(c == j) else centroids[j]
            for j in range(K)
        ])
        # record J
        history.append(compute_distortion(X, new_centroids, c))
        if np.allclose(new_centroids, centroids):
            break
        centroids = new_centroids
    return centroids, c, history
```

### 8. Practice Problems

1. **Objective Decay Plot**
    - Generate 500 points from two blobs.
    - Run the above `kmeans_with_history`.
    - Plot `history` vs iteration number. Verify J monotonically decreases.
2. **Effect of Initialization**
    - On a synthetic dataset, run K-means 10 times with random init and 10 times with K-means++.
    - Record final J for each run. Compare distributions of final J.
3. **Cluster Quality vs J**
    - For K=2…6, record J and compute silhouette score (use `sklearn.metrics.silhouette_score`).
    - Plot both metrics vs K. Discuss trade-offs in choosing K.

---

## Initializing K-Means

### Prerequisites

- Sampling from a uniform or weighted distribution
- Computing squared distances between points and centroids
- Basic NumPy operations

### Why Initialization Matters

Every run of K-means finds a local minimum of the distortion cost. Poor initial centroids can lead to:

- Convergence to bad clusters (high distortion)
- Empty clusters if no points are assigned
- Slower convergence (more iterations)

Smart initialization boosts both clustering quality and speed.

### 1. Random Initialization

### Steps

1. Pick K distinct indices from `[0…m-1]` uniformly at random.
2. Set each centroid `mu[j]` to the corresponding data point.

```python
import numpy as np

def init_random(X, K, seed=None):
    np.random.seed(seed)
    m = X.shape[0]
    choices = np.random.choice(m, size=K, replace=False)
    centroids = X[choices]
    return centroids
```

### Pros & Cons

- Pros: trivial to implement, O(K) time
- Cons: often yields poor starting points, high variance in final J

### 2. K-Means++ Initialization

K-means++ seeds centroids to spread them out:

1. Choose the first centroid uniformly from X.
2. For each point x, compute
    
    ```python
    D(x) = min_j || x - mu[j] ||**2
    ```
    
3. Sample the next centroid from X with probability proportional to D(x).
4. Repeat step 2–3 until you have K centroids.

```python
def init_kmeans_pp(X, K, seed=None):
    np.random.seed(seed)
    m, _ = X.shape
    centroids = []
    # 1. First centroid
    first_idx = np.random.randint(m)
    centroids.append(X[first_idx])
    # 2. Choose remaining
    for _ in range(1, K):
        dists = np.min(
            np.linalg.norm(X[:, None] - np.array(centroids), axis=2)**2,
            axis=1
        )
        probs = dists / dists.sum()
        next_idx = np.random.choice(m, p=probs)
        centroids.append(X[next_idx])
    return np.array(centroids)
```

### Benefits

- Often reduces final distortion by 30–50% vs random init
- Speeds up convergence (fewer iterations)
- Theoretical guarantee: expected cost ≤ O(log K)·optimal

### 3. Advanced Initialization (Overview)

- **K-Means||**: Scalable parallel seeding for very large datasets
- **Global K-Means**: Deterministic, adds one centroid at a time optimizing J
- **Density-Based Seeding**: Bias seeds toward high-density regions

### 4. Comparison

| Method | Quality of Init | Time Complexity | When to Use |
| --- | --- | --- | --- |
| Random | Low | O(K) | Very small datasets or quick tests |
| K-Means++ | High | O(K·m) | General-purpose clustering |
| K-Means |  |  | High, parallelizable |

### 5. Geometric Insight

- Random init picks centroids that might all sit in one dense region.
- K-means++ pushes each new seed away from existing ones in proportion to squared distance, ensuring coverage of all “islands” of data.

### 6. Practice Problems

1. **Implement and Compare**
    - Generate 4 Gaussian blobs (use `make_blobs`) in 2D.
    - Initialize centroids with random and K-means++.
    - Plot initial positions over data; compute initial distortion J before running K-means.
2. **Seed Sensitivity Study**
    - On a fixed dataset, run K-means 20 times with random init and 20 times with K-means++.
    - Record final J for each run.
    - Use boxplots to compare variance of distortion.
3. **Large-Scale Seeding (Optional)**
    - Implement a simplified K-Means||:
        1. Sample an initial small set of candidates with K-means++ logic.
        2. Run one pass of K-means on those candidates to pick final K.
    - Test on a 100k-point dataset; compare runtime vs standard K-means++.

---

## Choosing the Number of Clusters (K)

### Prerequisites

- Understanding of K-means distortion (inertia)
- Familiarity with Euclidean distance
- Basic plotting in Python (matplotlib or seaborn)
- Concept of model bias–variance trade-off

### 1. Why Picking K Matters

Every cluster you introduce is like adding a new “model complexity” knob.

- Too few clusters → underfitting: you lump distinct groups together, losing nuance.
- Too many clusters → overfitting: you carve noise into its own cluster, reducing generality.

Goal: find the sweet spot where clusters capture meaningful structure without chasing random variation.

### 2. Elbow Method

### Intuition

Plot the total within-cluster sum of squares (inertia) versus K. The curve will drop steeply at first, then flatten. The “elbow” point balances fit versus complexity.

### Formula for Inertia

```python
# X: array of shape (m, n)
# centroids: array of shape (K, n)
# assignments: array of shape (m,), assignment[i] ∈ {0…K-1}

def compute_inertia(X, centroids, assignments):
    inertia = 0
    for i in range(X.shape[0]):
        j = assignments[i]
        inertia += np.sum((X[i] - centroids[j])**2)
    return inertia
```

### Steps

1. For each K in a range (e.g., 1…10):
    - Run K-means, record inertia.
2. Plot `K` vs `inertia`.
3. Choose the K at which the rate of decrease sharply changes (“elbow”).

### 3. Silhouette Analysis

### Intuition

Measures how well each point fits its cluster versus the next best cluster. Values range from −1 to +1:

- Near +1: well matched to own cluster, far from neighbors.
- Near 0: on the boundary between two clusters.
- Negative: probably assigned to wrong cluster.

### Silhouette Score Formula (per sample)

```python
# a = average distance to all other points in same cluster
# b = minimum average distance to points in any other cluster

silhouette_i = (b - a) / max(a, b)
```

### Steps

1. For each K, fit K-means and compute the average silhouette score over all points:
    
    ```python
    from sklearn.metrics import silhouette_score
    score = silhouette_score(X, labels)
    ```
    
2. Plot K vs average silhouette score.
3. Pick K with the highest average score.

### 4. Gap Statistic

### Intuition

Compares your data’s clustering dispersion to that of data with no real clusters (uniform reference). A large gap suggests strong cluster structure.

### Gap Formula

```python
# Wk = inertia for real data at K
# Wk_ref[i] = inertia on i-th reference dataset at K (B times)
import numpy as np

gap_K = np.mean(np.log(Wk_ref)) - np.log(Wk)
sk = np.sqrt(1 + 1/B) * np.std(np.log(Wk_ref))
```

### Steps

1. For each K:
    - Compute Wk on your data.
    - Generate B reference datasets (uniform in same bounds), compute Wk_ref.
    - Calculate gap_K and its standard error sk.
2. Choose the smallest K such that:
    
    ```
    gap_K ≥ gap_{K+1} - s_{K+1}
    ```
    

### 5. Information Criteria (Model-Based)

When using Gaussian Mixture Models (GMM), you can compare BIC/AIC:

```python
from sklearn.mixture import GaussianMixture

# for each K
gmm = GaussianMixture(n_components=K).fit(X)
bic = gmm.bic(X)
aic = gmm.aic(X)
```

Pick the K with lowest BIC (or AIC), balancing data fit against model complexity.

### 6. Practical Tips

- Always **standardize** features first.
- Combine methods: look for agreement between elbow, silhouette, and gap.
- Inject **domain knowledge**: if marketing tells you 4 customer segments, bias toward K≈4.
- Beware “smooth elbows”: inertia often decreases smoothly without a clear hinge.
- Check **cluster sizes**: avoid tiny clusters (<1% of data) unless they’re real outliers.

### 7. Visualization Techniques

- **Elbow plot**: `plt.plot(K_values, inertias, '-o')`
- **Silhouette plot**:
    - Use `sklearn.metrics.silhouette_samples` to plot each sample’s score by cluster.
- **Gap curve**: plot `gap_K` ± `sk` error bars against K.

Visual cues often expose the best K more clearly than raw numbers.

### 8. Practice Problems

1. **Elbow & Silhouette on Synthetic Blobs**
    - Generate 400 points from four 2D blobs.
    - Compute inertia and silhouette score for K=1…8.
    - Plot both curves side by side. Interpret where they agree or conflict.
2. **Implement Gap Statistic from Scratch**
    - Use B=20 reference datasets.
    - For K=1…6, compute `gap_K` and standard error `sk`.
    - Plot gap curve with error bars.
    - Verify the K selection rule.
3. **BIC with GMM vs K-Means**
    - On the Iris dataset, fit GMMs for K=1…6, record BIC.
    - Compare with elbow and silhouette results from K-means.
    - Discuss which method you’d trust and why.

---

## Anomaly Detection (Finding Unusual Events)

### 1. Prerequisites

- Basic probability and Gaussian (normal) distribution
- Computing mean and variance (or covariance matrix)
- Logarithms for numerical stability
- NumPy for vector/matrix operations

Quick refresher on sample mean and variance:

```python
# X: array of shape (m,), univariate data
mu = np.mean(X)
sigma2 = np.var(X)        # variance: E[(X - mu)^2]
```

### 2. Intuition & Analogy

Think of monitoring the temperature of a machine bearing. Most of the time it runs between 40 °C and 60 °C. Suddenly it spikes to 80 °C—an anomaly indicating possible failure.

Anomaly detection models “normal” behavior as a probability distribution. Points that have very low probability under that model are flagged as unusual.

### 3. Univariate Gaussian Model

### 3.1 Model Definition

Assume each feature x_j of a data point x ∈ ℝⁿ is independent and normally distributed:

```python
p(x) = ∏_{j=1..n}  1 / sqrt(2π σ2_j) * exp( - (x_j - μ_j)**2 / (2 σ2_j) )
```

### 3.2 Parameter Estimation

Estimate μ_j and σ2_j from “normal” training data (no anomalies):

```python
mu = np.mean(X_train, axis=0)           # shape (n,)
sigma2 = np.var(X_train, axis=0)        # shape (n,)
```

### 3.3 Scoring & Thresholding

For each new example x:

```python
def gaussian_probability(x, mu, sigma2):
    coeff = 1.0 / np.sqrt(2 * np.pi * sigma2)
    exps  = np.exp(- (x - mu)**2 / (2 * sigma2))
    return np.prod(coeff * exps)

p = gaussian_probability(x, mu, sigma2)
```

Choose a threshold ε via cross-validation: flag as anomaly if `p < ε`.

### 4. Multivariate Gaussian Model

When features are correlated, model the joint distribution:

```python
p(x) = 1 / ((2π)^(n/2) * det(Sigma)^(1/2)) \
       * exp( -0.5 * (x - mu).T @ inv(Sigma) @ (x - mu) )
```

Estimate parameters from training data:

```python
mu = np.mean(X_train, axis=0)                         # (n,)
Sigma = np.cov(X_train, rowvar=False, bias=True)      # (n, n)
```

Compute probability safely using log-probability:

```python
import numpy as np

def multivariate_gaussian_logpdf(x, mu, Sigma):
    n = mu.size
    diff = x - mu
    sign, logdet = np.linalg.slogdet(Sigma)
    term1 = -0.5 * (diff @ np.linalg.inv(Sigma) @ diff)
    term2 = -0.5 * (n * np.log(2*np.pi) + logdet)
    return term1 + term2

logp = multivariate_gaussian_logpdf(x, mu, Sigma)
p = np.exp(logp)
```

Regularize Σ if singular:

```python
eps = 1e-6
Sigma += np.eye(n) * eps
```

Again pick threshold ε on log-p or p via cross-validation.

### 5. Geometric & Visual Insights

- **Univariate**: normal curve; anomalies lie in the tails beyond some cutoff.
- **Multivariate**: constant-probability contours are ellipsoids centered at μ. Points far from the ellipsoid’s center are anomalies.

Plotting in 2D:

```python
# contour of multivariate Gaussian
x1, x2 = np.meshgrid(np.linspace(...), np.linspace(...))
grid = np.stack([x1.ravel(), x2.ravel()], axis=1)
logp_grid = np.array([multivariate_gaussian_logpdf(pt, mu, Sigma) for pt in grid])
plt.contour(x1, x2, logp_grid.reshape(x1.shape))
```

### 6. Real-World Applications

- Credit card fraud detection (transactions far from usual spending patterns)
- Network intrusion (unusual traffic volumes or packet features)
- Predictive maintenance (vibration, temperature spikes)
- Healthcare monitoring (vital signs deviating sharply)

### 7. Practical Anomaly Detection Workflow

1. **Collect “normal” data** (no known anomalies).
2. **Engineer features** that capture relevant signals.
3. **Fit Gaussian model** (univariate or multivariate).
4. **Select threshold ε** using a held-out validation set labeled normal/anomaly.
5. **Score new examples**; flag those with `p < ε`.
6. **Evaluate performance** using precision, recall, and F1 on labeled data.

### 8. Practice Problems

1. **CPU Usage Monitoring (Univariate)**
    - Load a log of CPU usage percentages over time.
    - Fit a univariate Gaussian on the first 80% of data.
    - On the remaining 20%, manually label a few spikes as anomalies.
    - Find ε by maximizing F1 on this validation set.
2. **Server Metrics (Multivariate)**
    - Download a small dataset with two features: CPU and memory usage.
    - Fit a multivariate Gaussian; plot ellipsoidal contours and data points.
    - Flag anomalies and visualize them overlaid on the contour plot.
3. **Threshold Selection Script**
    - Write a function that, given `logp` scores and true labels, searches over ε grid to maximize F1.
    - Integrate it into your anomaly detection pipeline.
4. **Compare Methods**
    - On a 3D synthetic dataset with correlated features, apply univariate vs. multivariate detection.
    - Measure true positive and false positive rates for both. Discuss when multivariate is worth the extra complexity.

### 9. Beyond Gaussian Models

- **Robust Covariance / EllipticEnvelope**: estimates center and covariance with outlier-robust methods.
- **Local Outlier Factor (LOF)**: detects anomalies based on local density deviations.
- **One-Class SVM**: learns a boundary around normal data in feature space.

These methods can handle non-Gaussian data or complex distributions.

---

## Anomaly Detection Example: Multivariate Gaussian on Synthetic Data

### Prerequisites

- NumPy for array operations
- Matplotlib for plotting
- scikit-learn for metrics (optional)
- Understanding of mean and covariance estimation

### 1. Generate Synthetic Data

We’ll create a 2D “normal” cluster and inject anomalies from a uniform distribution.

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)

# Normal data: 300 points around (5, 5) with small covariance
mu_true = np.array([5.0, 5.0])
cov_true = np.array([[1.0, 0.6],
                     [0.6, 1.5]])
X_normal = np.random.multivariate_normal(mu_true, cov_true, size=300)

# Anomalies: 20 points uniformly in [0, 10]²
X_anomaly = np.random.uniform(low=0, high=10, size=(20, 2))

# Combine into one dataset and labels
X = np.vstack([X_normal, X_anomaly])
y_true = np.hstack([np.zeros(len(X_normal)), np.ones(len(X_anomaly))])

# Plot data
plt.figure(figsize=(6,6))
plt.scatter(X_normal[:,0], X_normal[:,1], label='Normal', alpha=0.6)
plt.scatter(X_anomaly[:,0], X_anomaly[:,1], label='Anomaly', alpha=0.8, color='red')
plt.legend()
plt.title('Synthetic Dataset')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
```

### 2. Estimate Parameters (μ and Σ)

Fit the multivariate Gaussian using only the “normal” points.

```python
# Estimate mean and covariance from normal data
mu_est = X_normal.mean(axis=0)             # shape (2,)
Sigma_est = np.cov(X_normal, rowvar=False) # shape (2, 2)

# Regularize covariance to avoid singular matrix
eps = 1e-6
Sigma_est += np.eye(2) * eps
```

### 3. Compute Log-Probability of Each Point

```python
import numpy.linalg as la

def multivariate_logpdf(x, mu, Sigma):
    n = mu.size
    diff = x - mu
    sign, logdet = la.slogdet(Sigma)
    invSigma = la.inv(Sigma)
    term = -0.5 * diff.dot(invSigma).dot(diff)
    norm = -0.5 * (n * np.log(2 * np.pi) + logdet)
    return term + norm

# Compute log-probs for all points
logps = np.array([multivariate_logpdf(x, mu_est, Sigma_est) for x in X])
```

### 4. Select Threshold ε via F1-Score

Use true labels to find ε that maximizes F1.

```python
from sklearn.metrics import f1_score

best_eps, best_f1 = 0, 0
eps_values = np.linspace(logps.min(), logps.max(), 1000)

for eps in eps_values:
    y_pred = (logps < eps).astype(int)
    score = f1_score(y_true, y_pred)
    if score > best_f1:
        best_f1, best_eps = score, eps

print(f'Best ε = {best_eps:.2f}, Best F1 = {best_f1:.3f}')
```

### 5. Flag Anomalies and Visualize

```python
# Final prediction
y_pred = (logps < best_eps).astype(int)

# Plot decision boundary (contour) and anomalies
xx, yy = np.meshgrid(
    np.linspace(0, 10, 200),
    np.linspace(0, 10, 200)
)
grid = np.c_[xx.ravel(), yy.ravel()]
grid_logps = np.array([multivariate_logpdf(pt, mu_est, Sigma_est) for pt in grid])
zz = grid_logps.reshape(xx.shape)

plt.figure(figsize=(6,6))
# Contour at threshold
plt.contour(xx, yy, zz, levels=[best_eps], colors='black', linewidths=1)
# Normal vs flagged anomalies
plt.scatter(X[y_pred==0,0], X[y_pred==0,1], label='Predicted Normal', alpha=0.6)
plt.scatter(X[y_pred==1,0], X[y_pred==1,1], label='Predicted Anomaly', color='red', alpha=0.8)
plt.legend()
plt.title('Anomaly Detection with Multivariate Gaussian')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
```

### 6. Explanation

- We modeled “normal” data as a 2D Gaussian parameterized by μ and Σ.
- The log-probability function returns high values near the center (μ) and low values in the tails.
- We chose ε by scanning thresholds and picking the one with highest F1 on labeled data.
- Points where `logp < ε` are flagged as anomalies.

This pipeline mirrors real-world workflows (fraud detection, server monitoring): fit on historical normal data, set threshold via cross-validation, then score new events in production.

### 7. Practice Extensions

- Repeat using a **univariate Gaussian** on a single feature (e.g., CPU usage).
- Implement **auto-thresholding** without labels (use percentiles of log-probabilities).
- Compare with **Local Outlier Factor** (`sklearn.neighbors.LocalOutlierFactor`).
- Apply on a real dataset (credit card fraud, network logs) and measure precision/recall.

---

## Gaussian (Normal) Distribution

### 1. Definition and PDF

A univariate normal distribution is fully described by its mean `mu` and variance `sigma2`. Its probability density function (PDF) at a point `x` is:

```
f(x; mu, sigma2) = 1 / sqrt(2 * pi * sigma2)
                   * exp( - (x - mu)^2 / (2 * sigma2) )
```

Every normal curve is symmetric around `mu` and shaped like a bell.

### 2. Cumulative Distribution Function (CDF)

The CDF gives the probability that a sample is ≤ `x`. It does not have a closed-form in elementary functions but uses the error function:

```
Phi(x; mu, sigma) = 0.5 * [1 + erf( (x - mu) / (sigma * sqrt(2)) )]
```

You’ll usually compute it via library calls (e.g., `scipy.stats.norm.cdf`).

### 3. Standard Normal and Z-Scores

Transform any normal variable `X ~ N(mu, sigma2)` into a standard normal `Z ~ N(0,1)`:

```
Z = (X - mu) / sigma
```

Z-scores let you look up probabilities in universal tables or fast implementations.

### 4. Key Properties

- The mean, median, and mode all equal `mu`.
- About 68% of values lie within `mu ± sigma`; 95% within `mu ± 2*sigma`; 99.7% within `mu ± 3*sigma`.
- Any linear shift/scale of a normal remains normal.
- Sums of independent normals are normal.

### 5. Moments and Characteristic Function

First few moments of `X ~ N(mu, sigma2)`:

| Moment | Expression |
| --- | --- |
| Mean | `mu` |
| Variance | `sigma2` |
| Third central moment | `0` |
| Fourth central moment | `3 * sigma2^2` |

The characteristic function φ(t) is:

```
phi(t) = exp( i * mu * t - 0.5 * sigma2 * t^2 )
```

### 6. Maximum Likelihood Estimation (MLE)

Given data `x1…xn`, the MLE estimates are:

```
mu_hat    = (1/n) * sum_{i=1..n} xi

sigma2_hat = (1/n) * sum_{i=1..n} (xi - mu_hat)^2
```

These maximize the log-likelihood over `mu` and `sigma2`.

### 7. Linear Transformations

If `Y = a * X + b` and `X ~ N(mu, sigma2)`, then:

```
Y ~ N( a * mu + b , (a^2) * sigma2 )
```

This underlies standardization, scaling, and affine transformations.

### 8. Multivariate Normal Distribution

A d-dimensional normal `X ~ N(mu, Sigma)` has PDF:

```
f(x) = 1 / ((2*pi)^(d/2) * det(Sigma)^(1/2))
       * exp( -0.5 * (x - mu)^T * inv(Sigma) * (x - mu) )
```

Key points:

- Contours of equal density are ellipsoids.
- `mu` is a d-vector; `Sigma` is a d×d positive-definite covariance matrix.

### 9. Geometry and Mahalanobis Distance

The Mahalanobis distance from `x` to the mean is:

```
D(x) = sqrt( (x - mu)^T * inv(Sigma) * (x - mu) )
```

Points with constant `D(x)` lie on ellipsoidal contours aligned with `Sigma`’s eigenvectors.

### 10. Marginals and Conditionals

For a partitioned vector `X = [X1; X2]`:

- Marginal of `X1` is normal with its own sub-mean and sub-covariance.
- Conditional `X1 | X2 = x2` is normal with:

```
mu_cond = mu1 + Sigma12 * inv(Sigma22) * (x2 - mu2)

Sigma_cond = Sigma11 - Sigma12 * inv(Sigma22) * Sigma21
```

### 11. Exponential-Family Form

Univariate normal can be written as:

```
f(x) = h(x) * exp( eta1 * x + eta2 * x^2 - A(eta) )
```

where natural parameters `eta1 = mu / sigma2`, `eta2 = -1/(2*sigma2)`. This view unifies many distributions.

### 12. KL Divergence Between Two Gaussians

For `P = N(mu0, sigma0^2)` and `Q = N(mu1, sigma1^2)`:

```
KL(P || Q) = 0.5 * [
    (sigma0^2 / sigma1^2)
  + ((mu1 - mu0)^2 / sigma1^2)
  - 1
  + 2 * log(sigma1 / sigma0)
]
```

This measures how “far” two normals are in information space.

### 13. Gaussian Mixture Models (GMM)

A weighted sum of K Gaussians:

```
p(x) = sum_{k=1..K} pi_k * N(x | mu_k, Sigma_k)
```

Fitted via the Expectation-Maximization (EM) algorithm. Captures multimodal data.

### 14. Bayesian Inference with Gaussians

- Conjugate prior: Normal likelihood with known variance and normal prior on the mean yields a normal posterior.
- Posterior parameters are weighted averages of prior and data statistics.

### 15. Gaussian Processes (GP)

A GP is an infinite-dimensional generalization of the multivariate normal. Every finite subset of function values has a joint Gaussian distribution. Defined by a mean function `m(x)` and covariance kernel `k(x, x')`.

### 16. Central Limit Theorem (CLT)

The sum (or average) of many independent, finite-variance random variables tends toward a normal distribution, regardless of the original distributions. This explains why normals appear ubiquitously.

### 17. Practical Considerations

- Real data often deviates in tails (heavy tails, skew).
- Robust alternatives: Student’s t, mixture models, nonparametric density estimates.
- In high dimensions, covariance estimation needs regularization (shrinkage, sparsity).

---

## Anomaly Detection Algorithms

### 1. Problem Definition

Anomaly detection seeks to flag data points or patterns that deviate from expected behavior.

- Point anomalies: single events far from the norm.
- Contextual anomalies: normal values in abnormal contexts (e.g., spike in temperature at night).
- Collective anomalies: groups of points that jointly form an outlier (e.g., a sequence of unusual network packets).

### 2. Data and Evaluation

Before modeling, decide if you have labels (supervised) or only “normal” data (unsupervised).

- Metrics for labeled data: precision, recall, F1, ROC-AUC, PR-AUC.
- Unsupervised thresholding: choose a percentile of anomaly scores or set a contamination rate.

### 3. Simple Statistical Methods

### 3.1 Univariate Z-Score

Compute how many standard deviations each value is from the mean:

```
z_i = (x_i - mu) / sigma
```

Flag points where `|z_i|` exceeds a threshold (e.g., 3).

### 3.2 Multivariate Gaussian

Estimate mean vector `mu` and covariance `Sigma` using normal data, then compute log-likelihood:

```
logp(x) = -0.5 * [ (x-mu)^T * inv(Sigma) * (x-mu)
                  + d * log(2*pi)
                  + log(det(Sigma)) ]
```

Points with low `logp(x)` are anomalies.

### 3.3 Gaussian Mixture Model (GMM)

Model the density as a weighted sum of `K` Gaussians and use EM to fit:

```
p(x) = sum_{k=1..K} pi_k * N(x | mu_k, Sigma_k)
```

Anomaly score is `-log p(x)` under the mixture.

### 4. Proximity and Density Methods

### 4.1 k-Nearest Neighbors (kNN) Distance

For each point, compute the average distance to its `k` nearest neighbors. Large distances imply outliers.

### 4.2 Local Outlier Factor (LOF)

LOF compares local density of a point to densities of its neighbors. A higher LOF score means the point is more isolated.

### 4.3 Isolation Forest

Build random binary trees by subsampling features; anomalies require fewer splits to isolate:

```
score(x) = 2^{ - E(path_length(x)) / c(n) }
```

Lower path length → score near 1 → anomaly.

### 5. One-Class Classification

### 5.1 One-Class SVM

Fits a boundary around “normal” data in feature space using a kernel. Points outside the boundary are anomalies.

Key parameters:

- `nu`: approximate fraction of outliers.
- `kernel`: radial basis, polynomial, linear.

Decision function:

```
f(x) = sum_i alpha_i * K(x, x_i) - rho
```

Flag `f(x) < 0`.

### 6. Reconstruction-Based Deep Models

### 6.1 Autoencoder

Train an encoder–decoder to reconstruct normal data. Compute reconstruction error:

```
error(x) = || x - decoder( encoder(x) ) ||_2^2
```

Threshold errors to detect anomalies.

### 6.2 Variational Autoencoder (VAE)

Fits a probabilistic latent model. Uses ELBO loss; anomalies have low reconstructed likelihood.

### 6.3 GAN-Based Methods

Train a generator to mimic normal data and discriminator to distinguish real vs generated. Anomalies are poorly reconstructed or classified.

### 7. Sequence and Time-Series Methods

- **Predictive models**: LSTM or ARIMA to forecast next value; large prediction error signals anomaly.
- **Change point detection**: monitor shifts in mean/variance over time windows.
- **Matrix/Tensor decomposition**: nonnegative matrix factorization on sliding windows.

### 8. Threshold Selection Strategies

- **Supervised**: sweep threshold to maximize F1 or ROC-AUC on a validation set.
- **Unsupervised**: pick top `k%` lowest likelihoods or highest scores based on expected contamination.
- **Adaptive**: update threshold over a moving window of scores in production.

### 9. Comparison Table

| Method Category | Example Methods | Complexity | Pros | Cons |
| --- | --- | --- | --- | --- |
| Statistical | Z-score, Gaussian, GMM | Low | Simple, interpretable | Fails on non-Gaussian data |
| Proximity/Density | kNN, LOF, Isolation Forest | Medium | Nonparametric, few assumptions | Scalability on large n |
| Boundary/One-Class | One-Class SVM | High (kernel) | Solid theory, kernel flexibility | Sensitive to hyperparameters |
| Reconstruction/Deep | Autoencoder, VAE, GAN | High (GPU) | Model complex patterns | Needs large normal dataset |
| Sequence/Change-Point | LSTM-predict, ARIMA | Medium–High | Captures temporal context | Hard to tune for non-stationary data |

---

## Developing and Evaluating an Anomaly Detection System

### 1. Problem Definition

Define what “anomaly” means in your context.

- Point anomaly: an individual record far from the norm
- Contextual anomaly: normal values in an unusual context
- Collective anomaly: a sequence or group of records that together are abnormal

Clarify

- Objective (early‐warning, fraud flagging, root‐cause analysis)
- Data availability (only normal vs. labeled anomalies)

### 2. Data Collection & Labeling

Gather representative data for your target domain.

- Historical logs, sensor readings, transaction records
- If possible, curate a small labeled set of anomalies for evaluation
- If labels are impossible, reserve a clean “normal” period for validation

Key actions:

- Timestamp alignment and consistency checks
- Class imbalance awareness (anomalies usually <1%)

### 3. Data Preprocessing & Feature Engineering

Clean and transform raw inputs into features:

```
# Example: extract features from time series
#   - sliding window statistics
#   - trend and seasonality components
window = 50
rolling_mean = series.rolling(window).mean()
rolling_std  = series.rolling(window).std()
```

Common steps:

- Missing value imputation (forward/backward fill, interpolation)
- Scaling and normalization (MinMax, robust scaler)
- Aggregation (time windows, counts, ratios)
- Categorical encoding (one‐hot, target encoding)

### 4. Model Selection

Choose based on data size, dimensionality, and label availability:

1. **Statistical**
    - Z-score, univariate or multivariate Gaussian
2. **Proximity / Density**
    - k-NN distance, Local Outlier Factor, Isolation Forest
3. **One‐Class / Boundary**
    - One-Class SVM, Support Vector Data Description (SVDD)
4. **Reconstruction / Deep**
    - Autoencoder, Variational Autoencoder, GAN
5. **Domain‐Specific**
    - Time-series models (ARIMA, LSTM forecasting)
    - Change‐point detection

### 5. Training & Parameter Estimation

Fit your model on “normal” data (or full data if semi-supervised):

```
# Example: fit a multivariate Gaussian
mu_hat    = X_normal.mean(axis=0)
cov_hat   = np.cov(X_normal, rowvar=False) + np.eye(d)*1e-6
```

For methods with hyperparameters (k in kNN, nu in One-Class SVM):

- Use grid search on a held‐out subset of normal data
- Or leverage labeled anomalies (if available)

### 6. Scoring & Anomaly Score

Compute a score that ranks how anomalous each point is:

```
# Multivariate Gaussian log‐probability
def score(x):
    diff = x - mu_hat
    return -0.5 * (diff @ inv(cov_hat) @ diff.T
                   + d * np.log(2*np.pi)
                   + np.log(np.linalg.det(cov_hat)))
```

Higher score → more “normal.”

Invert or negate if necessary so larger → more anomalous.

### 7. Threshold Selection

Decide cutoff ε on scores to flag anomalies:

- **Supervised**: sweep ε to maximize F1, ROC‐AUC on labeled data
- **Unsupervised**: pick top k% anomalies or a percentile of scores
- **Adaptive**: update ε over a rolling window to adapt to drift

```
# Example: unsupervised percentile threshold
epsilon = np.percentile(scores, 100 * (1 - contamination_rate))
anomalies = scores < epsilon
```

### 8. Evaluation Metrics & Strategies

When you have labels:

- Precision, recall, F1, PR-AUC
- ROC-AUC (be cautious on imbalanced data)

When only normal labels exist:

- False positive rate at fixed coverage of normal data
- Qualitative inspection (case studies, root‐cause logs)

Cross‐validation:

- For time series, use expanding window splits
- For i.i.d. data, use k-fold but ensure anomalies don’t leak

### 9. Robustness & Model Validation

- Test on multiple anomaly scenarios (synthetic injections, real anomalies)
- Sensitivity analysis on hyperparameters
- Monitor precision/recall over time in production
- Stress‐test on edge cases (missing features, concept drift)

### 10. Deployment & Monitoring

1. **Real‐Time Scoring**
    - Batch vs. streaming (Kafka, Flink, Spark Structured Streaming)
2. **Alerting & Feedback Loop**
    - Integrate with dashboards (Grafana, Kibana)
    - Human‐in‐the‐loop to label new anomalies
3. **Retraining & Drift Detection**
    - Drift detectors on feature distributions
    - Scheduled or event‐driven retraining

### 11. End‐to‐End Python Example

```python
import numpy as np
from sklearn.ensemble import IsolationForest

# 1. Load and preprocess data
X = load_features()              # shape (n_samples, n_features)
X_norm = preprocess(X)           # scaling, imputation

# 2. Fit model (isolation forest)
model = IsolationForest(contamination=0.02, random_state=0)
model.fit(X_norm)

# 3. Score and threshold
scores = -model.decision_function(X_norm)
epsilon = np.percentile(scores, 98)  # top 2% anomalies
y_pred = (scores >= epsilon).astype(int)

# 4. Evaluate (if labels available)
print(classification_report(y_true, y_pred))
```

### 12. Pitfalls & Best Practices

- Beware overfitting to synthetic anomalies
- Validate on multiple anomaly types
- Regularize covariance in Gaussian methods
- Scale features to avoid dominance by large‐scale variables
- Document thresholds, model versions, and drift incidents

---

## Aircraft Engine Monitoring: End-to-End Anomaly Detection System

### 1. Problem Definition

In aircraft engine health monitoring, an anomaly is any deviation in sensor readings or derived features that may indicate developing faults.

- Point anomalies: sudden spikes or drops in a temperature or vibration sensor.
- Contextual anomalies: an RPM value that’s high only during descent, not cruise.
- Collective anomalies: a drifting pattern in oil pressure over many flight cycles.

Goals:

- Detect early signs of degradation before unscheduled maintenance.
- Minimize false alarms to avoid unnecessary inspections.
- Quantify “lead time” between anomaly flag and actual failure.

### 2. Data Description

We’ll use the NASA C-MAPSS turbofan engine dataset (run-to-failure simulations).

Key components:

- **Units**: 100 engines simulated to failure.
- **Cycles**: readings per engine per flight cycle (up to 300 cycles).
- **Operational settings**: 3 variables (altitude, flight conditions).
- **Sensors**: 21 measurements (temperatures, pressures, vibrations).

Data splits:

- **Training set**: first N healthy cycles of each engine.
- **Test set**: all cycles, including near-failure.

### 3. Data Preprocessing & Feature Engineering

### 3.1 Clean and Align

- Drop sensors with constant or missing values.
- Align by cycle number across all engines.

### 3.2 Scaling

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)   # X_train shape (n_samples, n_features)
X_test_scaled  = scaler.transform(X_test)
```

### 3.3 Rolling-Window Features

Extract short-term and long-term trends per engine unit:

```python
import pandas as pd

df = pd.DataFrame(X_train_scaled, columns=feature_names)
df['cycle'] = cycle_numbers
df['unit']  = unit_ids

# For each unit and sensor, compute rolling mean/std over past 10 cycles
df['temp1_roll_mean'] = df.groupby('unit')['T2'].rolling(10).mean().reset_index(0,drop=True)
df['temp1_roll_std']  = df.groupby('unit')['T2'].rolling(10).std().reset_index(0,drop=True)
```

### 3.4 Derived Health Indicators

- **Rate of change**: difference between cycles
- **Shock counts**: count of high-vibration events in last window
- **Regression residuals**: fit a simple degradation trend and use residual as feature

### 4. Model Selection

Based on data size, real-time requirements, and interpretability:

1. **Multivariate Gaussian** (baseline)
2. **Isolation Forest** (fast, nonparametric)
3. **Autoencoder** (captures nonlinear interactions)

You can compare their detection lead times and false alarm rates.

### 5. Training on Healthy Data

### 5.1 Multivariate Gaussian Fit

```
mu_hat    = X_train_scaled.mean(axis=0)          # shape (d,)
cov_hat   = np.cov(X_train_scaled, rowvar=False) # shape (d, d)
cov_hat  += np.eye(d) * 1e-6                     # regularization
```

### 5.2 Isolation Forest Fit

```python
from sklearn.ensemble import IsolationForest

iso = IsolationForest(contamination=0.02, random_state=0)
iso.fit(X_train_scaled)
```

### 5.3 Autoencoder Fit

```python
from tensorflow.keras import layers, models

input_dim = X_train_scaled.shape[1]
encoding_dim = 16

# Encoder
inputs = layers.Input(shape=(input_dim,))
encoded = layers.Dense(encoding_dim, activation='relu')(inputs)

# Decoder
decoded = layers.Dense(input_dim, activation='sigmoid')(encoded)

autoencoder = models.Model(inputs, decoded)
autoencoder.compile(optimizer='adam', loss='mse')

autoencoder.fit(
    X_train_scaled, X_train_scaled,
    epochs=50, batch_size=64,
    validation_split=0.1
)
```

### 6. Scoring New Cycles

### 6.1 Gaussian Log-Probability

```
def logp_gauss(x, mu, cov_inv, log_det_cov, d):
    diff = x - mu
    return -0.5 * ( diff @ cov_inv @ diff.T
                  + d * np.log(2 * np.pi)
                  + log_det_cov )
```

Pre-compute `cov_inv = inv(cov_hat)` and `log_det_cov = log(det(cov_hat))`.

### 6.2 Isolation Forest Score

```python
scores_iso = -iso.decision_function(X_test_scaled)
```

Larger → more anomalous.

### 6.3 Autoencoder Reconstruction Error

```python
X_pred = autoencoder.predict(X_test_scaled)
errors = np.mean((X_test_scaled - X_pred)**2, axis=1)
```

Higher error → more anomalous.

### 7. Threshold Selection

### 7.1 Using Held-Out Failures

If we reserve some engines with labeled failure cycles:

```python
from sklearn.metrics import f1_score

best_eps, best_f1 = None, 0
for eps in np.linspace(min(scores), max(scores), 200):
    y_pred = (scores >= eps).astype(int)
    f1 = f1_score(y_true, y_pred)
    if f1 > best_f1:
        best_f1, best_eps = f1, eps
```

### 7.2 Unsupervised Percentile

```
eps = np.percentile(scores_train, 100 * (1 - contamination_rate))
```

### 8. Evaluation Metrics

- **Detection lead time**: cycles between flag and actual failure.
- **Precision / Recall / F1** on flagged cycles.
- **False alarm rate** per 1,000 cycles.
- **ROC-AUC** (for ranking).

Plot recall vs. lead time to measure early detection performance.

### 9. Cross-Validation Strategy

- **Time-series split**: train on early cycles, test on later cycles of the same units.
- **Leave-one-unit-out**: train on all but one engine, test on held-out engine.

### 10. Deployment Architecture

1. **Data Ingestion**: stream cycle data via Kafka or MQTT.
2. **Feature Service**: compute rolling features in real time (window store).
3. **Model Service**: REST endpoint hosting anomaly model (Gaussian, ISO, AE).
4. **Alerting**: push anomalies to dashboard (Grafana) or pager system.
5. **Feedback Loop**: engineer labels returned to refine threshold and retraining.

### 11. Monitoring & Maintenance

- Track **drift** in sensor distributions; retrain when data shifts.
- Automate **retraining pipeline** monthly or after significant events.
- Log all flags and engineer investigations to update labels.

---

## Anomaly Detection vs Supervised Learning

### 1. Overview

Anomaly detection and supervised learning both aim to identify patterns in data but differ in problem setup, data requirements, and evaluation.

Anomaly detection focuses on modeling “normal” behavior and flags deviations, often with few or no labeled anomalies.

Supervised learning trains a model to discriminate between predefined classes using labeled examples for all classes.

### 2. Supervised Learning

### 2.1 Definition

In supervised learning, you have a dataset of input–output pairs `(x, y)` where `y` is known for every example. The goal is to learn a function `f(x)` that predicts `y` for new inputs.

### 2.2 Data Requirements

- Requires labels for every class, including the minority (anomalies).
- Training set must be representative of all classes.
- Imbalanced classes can degrade performance unless handled explicitly.

### 2.3 Common Algorithms

- Logistic Regression
- Decision Trees / Random Forest
- Support Vector Machines
- Neural Networks

### 2.4 Evaluation Metrics

- Accuracy
- Precision, Recall, F1-Score
- ROC-AUC, PR-AUC

```
classification_error = count(y_pred != y_true) / n_samples
```

### 3. Anomaly Detection

### 3.1 Definition

Anomaly detection models “normal” data (often with no anomalies in training) and scores deviations. Points with scores beyond a threshold are flagged as anomalies.

### 3.2 Data Requirements

- Labeled normal data only (semi-supervised) or no labels at all (unsupervised).
- Rare or unknown anomalies make collecting labeled outliers impractical.
- Often assumes contamination rate (expected fraction of anomalies) for thresholding.

### 3.3 Common Algorithms

- Statistical methods (Z-score, Gaussian models)
- Density/proximity methods (k-NN, LOF, Isolation Forest)
- One-Class SVM
- Reconstruction models (Autoencoders, VAEs)

### 3.4 Evaluation Metrics

- Precision, Recall, F1-Score (if some anomaly labels exist)
- False Positive Rate on held-out normal data
- ROC-AUC on anomaly score ranking
- Detection lead time in time-series contexts

### 4. Key Differences

| Aspect | Supervised Learning | Anomaly Detection |
| --- | --- | --- |
| Labels | Required for every class | Often only normal class labels |
| Problem Type | Classification or regression | Outlier or novelty detection |
| Model Output | Predicted class or continuous value | Anomaly score (higher → more anomalous) |
| Thresholding | Decision boundary learned directly | Threshold set post-training |
| Handling Imbalance | Needs special techniques (resampling) | Naturally designed for rare class |

### 5. Algorithms & Techniques

### 5.1 Supervised Classification

- Fit `f(x; θ)` by minimizing loss over `(x_i, y_i)`
- Example: logistic loss
    
    ```
    L(θ) = - sum_{i=1..n} [
              y_i * log σ(θᵀ x_i)
            + (1-y_i) * log (1 - σ(θᵀ x_i))
           ]
    ```
    
- Directly outputs class probabilities or labels

### 5.2 Unsupervised Anomaly Scoring

- Estimate normal data distribution `p_normal(x)` or local density
- Score `s(x) = - log p_normal(x)` or distance metric
- Flag `x` as anomaly if `s(x) > ε`

### 6. Data Requirements & Labeling Effort

- Supervised: heavy labeling cost for all classes; may need synthetic oversampling for rare events (SMOTE).
- Anomaly Detection: minimal labeling (only normal); anomalies detected one-class or without labels.

### 7. Evaluation Strategies

- Supervised: cross-validation on balanced or stratified folds.
- Anomaly:
    - If small anomaly labels exist, optimize threshold by F1 or PR-AUC.
    - Otherwise, select threshold on held-out normal data (e.g., 99th percentile of scores).

### 8. Advantages & Limitations

### 8.1 Supervised Learning

- Advantages
    - High accuracy when ample labeled data available
    - Direct optimization of classification error
- Limitations
    - Poor generalization to unseen anomaly types
    - Expensive to label rare events

### 8.2 Anomaly Detection

- Advantages
    - Detects novel or unknown anomalies
    - Requires only normal data for training
- Limitations
    - Hard to set threshold without labels
    - Prone to false alarms if normal data is non-stationary

### 9. Use Cases

- Supervised
    - Spam detection with labeled spam/non-spam emails
    - Image classification with abundant labeled images
- Anomaly Detection
    - Fraud detection in payment streams
    - Machine health monitoring (engine vibration)
    - Network intrusion detection

---

## Feature Selection & Error Analysis for Data Center Anomaly Detection

### 1. Domain Understanding and Feature Identification

Begin by mapping out what can go wrong in your data center (hardware faults, software bugs, capacity spikes).

- Talk to SREs and ops teams to list critical metrics
- Inventory available data sources: system metrics, application logs, network flows
- Define failure modes you want to catch early (CPU hot‐loops, disk queue buildup, memory leaks)

### 2. Key Feature Types for Computer Monitoring

Capture both instantaneous readings and temporal behavior:

- **Resource Utilization**
    - CPU usage (%), memory usage (GB), disk I/O ops/sec, network bytes/sec
- **Queue and Latency Metrics**
    - Disk queue length, request latency percentiles (p50, p95, p99)
- **Error Counts and Logs**
    - System error rates, HTTP 5xx counts, exception log frequency
- **Topology and Configuration**
    - Machine role (db, web), rack location, VM versus bare‐metal

### 3. Handling Non-Gaussian Feature Distributions

Many monitoring signals are heavy-tailed or multi-modal. Use transformations or non-parametric methods:

- **Power or Log Transform**
    
    ```
    x_transformed = log(x + eps)
    ```
    
- **Quantile or Rank Transform**
    
    ```python
    from sklearn.preprocessing import QuantileTransformer
    qt = QuantileTransformer(output_distribution='normal')
    X_qt = qt.fit_transform(X)
    ```
    
- **Binning and Categorical Encoding**
    - Convert high-cardinality values into percentile bins or categories

### 4. Feature Engineering Techniques

Craft features that capture trends and anomalies over time:

- **Rolling Statistics**
    - mean, std, min, max over past N minutes or cycles
- **Differencing and Rates**
    
    ```
    delta_cpu = cpu_t - cpu_{t-1}
    rate_bytes = (bytes_t - bytes_{t-1}) / time_delta
    ```
    
- **Windowed Histograms / Entropy**
    - Count of distinct processes per window, entropy of HTTP status codes
- **Cross-Metric Ratios**
    - cpu_usage / cpu_capacity, error_count / request_count

### 5. Feature Selection Strategies

Filter down to the most informative features before modeling:

- **Unsupervised Filter**
    - Remove near-constant or highly correlated metrics
    - Keep features with high variance or kurtosis
- **Wrapper Methods with Anomaly Scores**
    - Iteratively add/drop features to optimize F1 or ROC-AUC on validation anomalies
- **Embedded Selection via Models**
    - Use tree-based feature importances (Isolation Forest, random forest)
    - L1-regularized one-class SVM to zero out irrelevant features

### 6. Error Analysis for Anomaly Detection

Iterate on your system by examining false positives and negatives:

- **False Positive Review**
    - Group by feature values and time to find noisy signals
    - Add or adjust filters (e.g., ignore spikes during planned maintenance)
- **False Negative Investigation**
    - Identify missed failure cases; correlate with raw features
    - Introduce new derived features (e.g., long-term drift)
- **Visualization & Dashboards**
    - Plot anomaly score timelines alongside key metrics
    - Use heatmaps or pairwise scatter plots for misclassified points

### 7. End-to-End Monitoring Pipeline

1. **Data Ingestion**
    - Pull metrics via Prometheus, Telegraf, or custom exporters
2. **Feature Computation**
    - Real-time sliding windows in Spark Streaming or Flink
3. **Anomaly Scoring**
    - Batch scoring for historical analysis; stream scoring for alerts
4. **Thresholding & Alerts**
    - Supervised: tune on labeled incidents; unsupervised: set percentile cutoffs
5. **Feedback Loop**
    - Log true/false alarms, retrain or adjust features monthly.

---